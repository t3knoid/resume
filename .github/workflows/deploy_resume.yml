name: Generate PDF of Resume and upload to Azure Storage

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'lighthouse-report.html'
      - 'README.md'
      - '.venv/**'
      - '.vscode/**'
  workflow_dispatch:

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      # Step 3: Install dependencies
      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Generate ATS-friendly resume HTML
      - name: Generate ATS-friendly Resume HTML
        run: python3 scripts/html_to_ats_resume.py

      # Step 5: Generate PDF of standard resume and upload to Azure Storage
      - name: Generate and Upload PDF
        id: pdfgen
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}
        run: python3 scripts/htmtopdf.py

      # Step 6: Generate PDF of ATS-friendly resume and upload to Azure Storage
      - name: Generate and Upload ATS-friendly PDF
        id: pdfgen_ats
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          AZURE_STORAGE_CONTAINER: ${{ secrets.AZURE_STORAGE_CONTAINER }}
        run: python3 scripts/htmtopdf.py --suffix resume_ats --html resume_ats.html

      # Step 7: Update robots.txt
      - name: Update robots.txt
        run: |
          dated="${{ steps.pdfgen.outputs.dated_pdf }}"
          dated_ats="${{ steps.pdfgen_ats.outputs.dated_pdf }}"
          latest="${{ steps.pdfgen.outputs.latest_pdf }}"
          latest_ats="${{ steps.pdfgen_ats.outputs.latest_pdf }}"

          echo "Updating robots.txt with:"
          echo "  $dated"
          echo "  $dated_ats"
          echo "  $latest"
          echo "  $latest_ats"

          # Create or update robots.txt to disallow crawlers from indexing dated and latest PDFs    
          echo "User-Agent: *" > robots.txt
          echo "Disallow: /$dated" >> robots.txt
          echo "Disallow: /$dated_ats" >> robots.txt
          echo "Disallow: /$latest" >> robots.txt
          echo "Disallow: /$latest_ats" >> robots.txt

      # Step 7: Commit and push changes to robots.txt, index.html, resume_ats.html
      - name: Commit and Push robots.txt, index.html, and resume_ats.html changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add robots.txt index.html resume_ats.html

          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update robots.txt, index.html, resume_ats.html"
            git push
          fi
